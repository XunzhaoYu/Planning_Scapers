Planning_Scapers


----- ----- ----- Project structure ----- ----- -----   
The project is structured as follows:  
root/  
â”œâ”€â”€ ðŸ“‚ Lists/:   The list of all applications from Local Authorities.   
â”œâ”€â”€ ðŸ“‚ ScrapedApplications/:   The data and documents scraped from the Local Authorities/PlanIt API.  
â”œâ”€â”€ ðŸ“‚ UKPlanning/:   All scripts/scrapers.  
â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ requirements.txt    
â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ general/:   General-purpose scraper logic (not tied to a specific framework).  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ base_scraper.py:   Common Scrapy Spider base class.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ utils.py:   General utility functions.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ items.py:   Define class Items for download files.   
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ parsers.py:   Shared parsing utilities.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚  
â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ scrapers/:   Individual scraper instances for each framework.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ Idox/  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ Idox1_scraper.py:   Idox scraper 1 (inherits from IdoxBaseSpider).  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ Idox2_scraper.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ ...  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ Atrium/  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ Atrium1_scraper.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ Atrium2_scraper.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ ...  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ PlanningExplorer/  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ PlanningExplorer1_scraper.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ ...  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ CCED_scraper.py # to be updated. 
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ Custom_scraper.py   
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ Tascomi.py         
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ Thames.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ others/  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â”œâ”€â”€ pdf_scraper.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â”œâ”€â”€ sitemap_scraper.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â””â”€â”€ ...   
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp;  
â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ middlewares/:   Globally available middleware modules.    
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ middlewares.py:   Base middlewares.   
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ middlewares_IP.py:   Middlewares for using IP proxies.     
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ middlewares_IP_rotation.py:   Middlewares for rotating IP proxies frequently.    
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ user_agent_mw.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ custom/  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â”œâ”€â”€ idox_proxy_mw.py:   Idox-specific custom middleware.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â””â”€â”€ atrium_auth_mw.py:   Atrium-specific custom middleware.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp;  
â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ pipelines/:   Globally available pipeline modules.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ pipelines.py:   Base pipelines.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ pipelines_extension.py:   Pipelines for obtaining file extensions.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ pipelines_form_extension.py:   Pipelines for obtaining file extensions with FormRequest.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ custom/  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â”œâ”€â”€ idox_custom_pipeline.py:   Idox-specific custom pipeline.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â””â”€â”€ atrium_custom_pipeline.py:   Atrium-specific custom pipeline.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚  
â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ tools/:   External tool modules.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ reCAPTCHA/  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ reCAPTCHA_model.py:   Data pre-processing, model training and prediction.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ reCAPTCHA_API.py:   APIs for scrapers.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ model/:   ML models for solving reCAPTCHA puzzles.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ ðŸ“„ image_classifier.h5     
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ raw_training/:   Raw training data before pre-processing.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ training/:   Training data.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ test/:   Test data.   
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ predicted/:   Prediction results of test data.   
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ deleted/:   Deleted duplicate training samples.   
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ ðŸ“„ class_names.txt  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ip_rotation/  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ rotator_proxy_service.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ rotator_custom_pool.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ data_process.py   
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ data_validation.py   
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ utils.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ email_sender.py:   Notification tools (Slack, email, etc. Available in local repository only).   
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp;  
â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“‚ configs/:   Project-wide configuration files.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ ðŸ“„ settings.py:   Global Scrapy settings.  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ frameworks/  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”œâ”€â”€ Idox_settings.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ Atrium_settings.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ scrapers/  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â”œâ”€â”€ Idox1_settings.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â”œâ”€â”€ Idox2_settings.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â””â”€â”€ Atrium1_settings.py  
â”‚ &nbsp; &nbsp; &nbsp; â”‚ &nbsp; &nbsp; &nbsp;  
â”‚ &nbsp; &nbsp; &nbsp; â””â”€â”€ tests/:   Unit and integration tests.  
â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â”œâ”€â”€ test_frameworks.py  
â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â”œâ”€â”€ test_scrapers.py  
â”‚ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; â””â”€â”€ test_middlewares.py  
â”œâ”€â”€ ðŸ“„ EC2_commands:   EC2 shell script for configuring EC2 instances.   
â”œâ”€â”€ ðŸ“„ local_commands:   Local shell script for configuring EC2 instances.  
â”œâ”€â”€ ðŸ“„ scraper_document.pdf:   User guidance for using scrapers on local machines (Scrapers).  
â”œâ”€â”€ ðŸ“„ scrapy.cfg:   Scrapy entry configuration.  
â””â”€â”€ ðŸ“„ README.md

----- ----- ----- Run scraper on local machines ----- ----- -----

See scraper_document.pdf for details.
 

----- ----- ----- Configure EC2 instances ----- ----- ----- 
1. Follow the instructions to start a new EC2 instance: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html

2. Get your EC2 instance's public IPv4 DNS, i.e: ec2-18-130-206-213
   <img src="https://github.com/XunzhaoYu/Planning_Scapers/blob/main/img/connection-prereqs-console2.png" width="75%">

4. Execute shell command from your local machine:
```
	python {your local command path}/local_commands.py {your EC2 instance's IPv4 DNS} init
```

5. Execute shell commands from your EC2 instance:
```
	python3 EC2_commands.py init
	source scraper_env/bin/activate
```

6. Execute shell command from your local machine:
```
	python {your local command path}/local_commands.py {your EC2 instance's IPv4 DNS} Data
```

7. Execute shell commands from your EC2 instance:
```
	python EC2_commands.py install_chromedriver
	python EC2_commands.py install_chrome
	python EC2_commands.py configure_env
	cd UKPlanning
	python main.py
```

----- ----- ----- Develop new scraper ----- ----- -----      
Currently, the scraper (UKPlanning_Scraper) is able to scrape most information items from Idox portals.         
To develop new scrapers by adapting the existing scraper, you can create a new scraper class as a subclass of UKPlanning_Scraper and overwrite its parse methods.     
           
           
--- --- END of UKPlanning_Scraper Guidance --- ---









           
```
###########        
Below is the guidance for UKPlanIt_API.py, not for local authorities.      Please ignore them.      
###########
```   
----- ----- ----- UKPlanIt APIs ----- ----- ----- 

File 'main.py' contains all APIs related to the scraper. Most APIs contain two parameters which are used to clarify the range of authorities to scrape or process. There are 424 authorities.

    scrape(start, end): To scrape data from the PlanIt API. Results are stored in 'Data_Temp'.
        i.e. scrape(2, 10) will scrape applications from the 2nd to the 10th authorities.
             scrape(5, 5) will scrape applications from the 5th authority.
    append_all(temp): append all csv files into a single csv file. By default, temp = 'True'.
        i.e. append_all(temp=True) will append all csv files in 'Data_Temp' folder.
             append_all(temp=False) will append all csv files in 'Data' folder.
    
    inverse(start, end): The scraped raw data is stored in an inverse order. This method will make applications in csv files stored in a chronological order. 'Data_Temp' -> 'Data_Temp'.
    append_by_year(start, end): append csv files from each authority by years. 'Data_Temp' -> 'Data'.


    
----- ----- ----- Quick start ----- ----- ----- 

Run the following pieces of code to get a csv file with applications from the first 10 authorities.
    
Option1:

	scrape(1, 10)

	append_all()
    
Option2:

	scrape(1, 10)

	inverse(1, 10)

	append_by_year(1, 10)

	append_all(False)

Two options will produce the same csv file named "UKPlanning.csv". But option2 will also produce many csv files in 'Data' folder, these files are useful for further comments and documents scraping. 






